spark支持application内的资源调度

# python的实现方式
1. 将spark.scheduler.mode设为FAIR.默认的FIFO会使得jobs按顺序执行,FAIR模式允许资源分成独立的部分,每个部分独立得执行各自的jobs.
2. 结合multiprocessing.pool.ThreadPool将不同的jobs组放入不同的线程,并把sparkContext.setLocalProperty('spark.scheduler.pool', pool)来显式得把任务放到相应的资源池.
