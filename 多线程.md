spark支持application内的资源调度

# python的实现方式
1. 将spark.scheduler.mode设为FAIR.默认的FIFO会使得jobs按顺序执行,FAIR模式允许资源分成独立的部分,每个部分独立得执行各自的jobs.
2. 结合multiprocessing.pool.ThreadPool将不同的jobs组放入不同的线程,并把sparkContext.setLocalProperty('spark.scheduler.pool', pool)来显式得把任务放到相应的资源池.
```python
from multiprocessing.pool import TheadPool

from pyspark.conf import SparkConf
from pyspark.sql import SparkSession 

spark: SparkSession = SparkSession.builder.config(conf=SparkConf().setExecutorEnv('', ''))\
    .appName("JimParker") \
    .enableHiveSupport() \
    .getOrCreate()
thread_pool = ThreadPool(processes=4)



def spark_thread_wrapper(args):
    spark.sparkContext.setLocalProperty("spark.scheduler.pool", args[1])
    spark_func(*args)

thread_pool.map(spark_thread_wrapper, [(spark, param1) for param1 in param1_list])
thread_pool.close()
thread_pool.join()
```
